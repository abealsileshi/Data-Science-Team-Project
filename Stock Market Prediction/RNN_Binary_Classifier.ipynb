{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9ezDPfFpjY-5"
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7WRs21ijctL"
   },
   "source": [
    "Loading in the data for several large companies, to be used as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "DMG0ofgDhpNf",
    "outputId": "d7a0aaba-b78d-48f5-dbf8-75e6fc315a30"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>TWTR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>24.861401</td>\n",
       "      <td>43.862000</td>\n",
       "      <td>523.373108</td>\n",
       "      <td>36.560001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>24.161013</td>\n",
       "      <td>42.018002</td>\n",
       "      <td>512.463013</td>\n",
       "      <td>36.380001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>24.163294</td>\n",
       "      <td>42.256001</td>\n",
       "      <td>500.585632</td>\n",
       "      <td>38.759998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>24.502111</td>\n",
       "      <td>42.189999</td>\n",
       "      <td>499.727997</td>\n",
       "      <td>37.279999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>25.443539</td>\n",
       "      <td>42.124001</td>\n",
       "      <td>501.303680</td>\n",
       "      <td>39.090000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AAPL       TSLA        GOOG       TWTR\n",
       "Date                                                   \n",
       "2015-01-02  24.861401  43.862000  523.373108  36.560001\n",
       "2015-01-05  24.161013  42.018002  512.463013  36.380001\n",
       "2015-01-06  24.163294  42.256001  500.585632  38.759998\n",
       "2015-01-07  24.502111  42.189999  499.727997  37.279999\n",
       "2015-01-08  25.443539  42.124001  501.303680  39.090000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = ['AAPL', 'TSLA', 'GOOG', 'TWTR'] \n",
    "target = 'AMZN'\n",
    "start = dt.datetime(2015, 1, 1)\n",
    "end = dt.datetime(2021, 1, 1)\n",
    "\n",
    "main_df = pd.DataFrame\n",
    "for ticker in tickers:\n",
    "    if ticker not in target: \n",
    "        df = web.DataReader(ticker, 'yahoo', start, end)\n",
    "        df = df[['Adj Close']]\n",
    "        df.rename(columns={'Adj Close': ticker}, inplace=True)\n",
    "    if main_df.empty: \n",
    "        main_df = df\n",
    "    else: \n",
    "        main_df = main_df.join(df, how='outer')\n",
    "\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pw4mi5OyseP-"
   },
   "source": [
    "Creating a few features for the target stock, namely the rate of increase in volume and adjusted close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hlssLHRenT1S"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>TWTR</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Moving_av</th>\n",
       "      <th>Increase_in_vol</th>\n",
       "      <th>Increase_in_adj_close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>24.861401</td>\n",
       "      <td>43.862000</td>\n",
       "      <td>523.373108</td>\n",
       "      <td>36.560001</td>\n",
       "      <td>314.750000</td>\n",
       "      <td>306.959991</td>\n",
       "      <td>312.579987</td>\n",
       "      <td>308.519989</td>\n",
       "      <td>2783200</td>\n",
       "      <td>308.519989</td>\n",
       "      <td>308.519989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>24.161013</td>\n",
       "      <td>42.018002</td>\n",
       "      <td>512.463013</td>\n",
       "      <td>36.380001</td>\n",
       "      <td>308.380005</td>\n",
       "      <td>300.850006</td>\n",
       "      <td>307.010010</td>\n",
       "      <td>302.190002</td>\n",
       "      <td>2774200</td>\n",
       "      <td>302.190002</td>\n",
       "      <td>305.354996</td>\n",
       "      <td>-9000.0</td>\n",
       "      <td>-6.329987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>24.163294</td>\n",
       "      <td>42.256001</td>\n",
       "      <td>500.585632</td>\n",
       "      <td>38.759998</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>292.380005</td>\n",
       "      <td>302.239990</td>\n",
       "      <td>295.290009</td>\n",
       "      <td>3519000</td>\n",
       "      <td>295.290009</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>744800.0</td>\n",
       "      <td>-6.899994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>24.502111</td>\n",
       "      <td>42.189999</td>\n",
       "      <td>499.727997</td>\n",
       "      <td>37.279999</td>\n",
       "      <td>301.279999</td>\n",
       "      <td>295.329987</td>\n",
       "      <td>297.500000</td>\n",
       "      <td>298.420013</td>\n",
       "      <td>2640300</td>\n",
       "      <td>298.420013</td>\n",
       "      <td>301.105003</td>\n",
       "      <td>-878700.0</td>\n",
       "      <td>3.130005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>25.443539</td>\n",
       "      <td>42.124001</td>\n",
       "      <td>501.303680</td>\n",
       "      <td>39.090000</td>\n",
       "      <td>303.140015</td>\n",
       "      <td>296.109985</td>\n",
       "      <td>300.320007</td>\n",
       "      <td>300.459991</td>\n",
       "      <td>3088400</td>\n",
       "      <td>300.459991</td>\n",
       "      <td>300.976001</td>\n",
       "      <td>448100.0</td>\n",
       "      <td>2.039978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AAPL       TSLA        GOOG       TWTR        High  \\\n",
       "Date                                                                  \n",
       "2015-01-02  24.861401  43.862000  523.373108  36.560001  314.750000   \n",
       "2015-01-05  24.161013  42.018002  512.463013  36.380001  308.380005   \n",
       "2015-01-06  24.163294  42.256001  500.585632  38.759998  303.000000   \n",
       "2015-01-07  24.502111  42.189999  499.727997  37.279999  301.279999   \n",
       "2015-01-08  25.443539  42.124001  501.303680  39.090000  303.140015   \n",
       "\n",
       "                   Low        Open       Close   Volume   Adj Close  \\\n",
       "Date                                                                  \n",
       "2015-01-02  306.959991  312.579987  308.519989  2783200  308.519989   \n",
       "2015-01-05  300.850006  307.010010  302.190002  2774200  302.190002   \n",
       "2015-01-06  292.380005  302.239990  295.290009  3519000  295.290009   \n",
       "2015-01-07  295.329987  297.500000  298.420013  2640300  298.420013   \n",
       "2015-01-08  296.109985  300.320007  300.459991  3088400  300.459991   \n",
       "\n",
       "             Moving_av  Increase_in_vol  Increase_in_adj_close  \n",
       "Date                                                            \n",
       "2015-01-02  308.519989              0.0               0.000000  \n",
       "2015-01-05  305.354996          -9000.0              -6.329987  \n",
       "2015-01-06  302.000000         744800.0              -6.899994  \n",
       "2015-01-07  301.105003        -878700.0               3.130005  \n",
       "2015-01-08  300.976001         448100.0               2.039978  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df = web.DataReader(target, 'yahoo', start, end)\n",
    "\n",
    "#calculate the moving average for a feature\n",
    "target_df['Moving_av'] = target_df['Adj Close'].rolling(window=50, min_periods=0).mean()\n",
    "\n",
    "#Calculate the rate of increase in volume and rate of increase in adjusted close \n",
    "i = 1\n",
    "rate_increase_in_vol = [0]\n",
    "rate_increase_in_adj_close = [0]\n",
    "\n",
    "while i < len(target_df):\n",
    "    rate_increase_in_vol.append(target_df.iloc[i]['Volume']-target_df.iloc[i - 1]['Volume'])\n",
    "    rate_increase_in_adj_close.append(target_df.iloc[i]['Adj Close']-target_df.iloc[i - 1]['Adj Close'])\n",
    "    i += 1\n",
    "\n",
    "target_df['Increase_in_vol'] = rate_increase_in_vol\n",
    "target_df['Increase_in_adj_close'] = rate_increase_in_adj_close  \n",
    "main_df = main_df.join(target_df, how='outer')\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YIzkMESnwwF"
   },
   "source": [
    "Adding a column preclose to each day, that holds the closing value for the previous day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3JNEr3x7oFUs",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>TWTR</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Moving_av</th>\n",
       "      <th>Increase_in_vol</th>\n",
       "      <th>Increase_in_adj_close</th>\n",
       "      <th>PrevClose</th>\n",
       "      <th>Return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>24.861401</td>\n",
       "      <td>43.862000</td>\n",
       "      <td>523.373108</td>\n",
       "      <td>36.560001</td>\n",
       "      <td>314.750000</td>\n",
       "      <td>306.959991</td>\n",
       "      <td>312.579987</td>\n",
       "      <td>308.519989</td>\n",
       "      <td>2783200</td>\n",
       "      <td>308.519989</td>\n",
       "      <td>308.519989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>24.161013</td>\n",
       "      <td>42.018002</td>\n",
       "      <td>512.463013</td>\n",
       "      <td>36.380001</td>\n",
       "      <td>308.380005</td>\n",
       "      <td>300.850006</td>\n",
       "      <td>307.010010</td>\n",
       "      <td>302.190002</td>\n",
       "      <td>2774200</td>\n",
       "      <td>302.190002</td>\n",
       "      <td>305.354996</td>\n",
       "      <td>-9000.0</td>\n",
       "      <td>-6.329987</td>\n",
       "      <td>308.519989</td>\n",
       "      <td>-0.020517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>24.163294</td>\n",
       "      <td>42.256001</td>\n",
       "      <td>500.585632</td>\n",
       "      <td>38.759998</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>292.380005</td>\n",
       "      <td>302.239990</td>\n",
       "      <td>295.290009</td>\n",
       "      <td>3519000</td>\n",
       "      <td>295.290009</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>744800.0</td>\n",
       "      <td>-6.899994</td>\n",
       "      <td>302.190002</td>\n",
       "      <td>-0.022833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>24.502111</td>\n",
       "      <td>42.189999</td>\n",
       "      <td>499.727997</td>\n",
       "      <td>37.279999</td>\n",
       "      <td>301.279999</td>\n",
       "      <td>295.329987</td>\n",
       "      <td>297.500000</td>\n",
       "      <td>298.420013</td>\n",
       "      <td>2640300</td>\n",
       "      <td>298.420013</td>\n",
       "      <td>301.105003</td>\n",
       "      <td>-878700.0</td>\n",
       "      <td>3.130005</td>\n",
       "      <td>295.290009</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>25.443539</td>\n",
       "      <td>42.124001</td>\n",
       "      <td>501.303680</td>\n",
       "      <td>39.090000</td>\n",
       "      <td>303.140015</td>\n",
       "      <td>296.109985</td>\n",
       "      <td>300.320007</td>\n",
       "      <td>300.459991</td>\n",
       "      <td>3088400</td>\n",
       "      <td>300.459991</td>\n",
       "      <td>300.976001</td>\n",
       "      <td>448100.0</td>\n",
       "      <td>2.039978</td>\n",
       "      <td>298.420013</td>\n",
       "      <td>0.006836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AAPL       TSLA        GOOG       TWTR        High  \\\n",
       "Date                                                                  \n",
       "2015-01-02  24.861401  43.862000  523.373108  36.560001  314.750000   \n",
       "2015-01-05  24.161013  42.018002  512.463013  36.380001  308.380005   \n",
       "2015-01-06  24.163294  42.256001  500.585632  38.759998  303.000000   \n",
       "2015-01-07  24.502111  42.189999  499.727997  37.279999  301.279999   \n",
       "2015-01-08  25.443539  42.124001  501.303680  39.090000  303.140015   \n",
       "\n",
       "                   Low        Open       Close   Volume   Adj Close  \\\n",
       "Date                                                                  \n",
       "2015-01-02  306.959991  312.579987  308.519989  2783200  308.519989   \n",
       "2015-01-05  300.850006  307.010010  302.190002  2774200  302.190002   \n",
       "2015-01-06  292.380005  302.239990  295.290009  3519000  295.290009   \n",
       "2015-01-07  295.329987  297.500000  298.420013  2640300  298.420013   \n",
       "2015-01-08  296.109985  300.320007  300.459991  3088400  300.459991   \n",
       "\n",
       "             Moving_av  Increase_in_vol  Increase_in_adj_close   PrevClose  \\\n",
       "Date                                                                         \n",
       "2015-01-02  308.519989              0.0               0.000000         NaN   \n",
       "2015-01-05  305.354996          -9000.0              -6.329987  308.519989   \n",
       "2015-01-06  302.000000         744800.0              -6.899994  302.190002   \n",
       "2015-01-07  301.105003        -878700.0               3.130005  295.290009   \n",
       "2015-01-08  300.976001         448100.0               2.039978  298.420013   \n",
       "\n",
       "              Return  \n",
       "Date                  \n",
       "2015-01-02       NaN  \n",
       "2015-01-05 -0.020517  \n",
       "2015-01-06 -0.022833  \n",
       "2015-01-07  0.010600  \n",
       "2015-01-08  0.006836  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df['PrevClose'] = main_df['Close'].shift(1)\n",
    "\n",
    "#changing key to integers\n",
    "key = np.array(np.arange(0, len(main_df)))\n",
    "main_df.set_index(key) \n",
    "\n",
    "#creating a col for the return values [(close - prev close)/prev close]\n",
    "main_df['Return'] = (main_df['Close'] - main_df['PrevClose'])/main_df['PrevClose']\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding sentiment to a copy of the main_df to compare the predictions of our model using average sentiment vs not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>TSLA</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>TWTR</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Moving_av</th>\n",
       "      <th>Increase_in_vol</th>\n",
       "      <th>Increase_in_adj_close</th>\n",
       "      <th>PrevClose</th>\n",
       "      <th>Return</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>24.861401</td>\n",
       "      <td>43.862000</td>\n",
       "      <td>523.373108</td>\n",
       "      <td>36.560001</td>\n",
       "      <td>314.750000</td>\n",
       "      <td>306.959991</td>\n",
       "      <td>312.579987</td>\n",
       "      <td>308.519989</td>\n",
       "      <td>2783200</td>\n",
       "      <td>308.519989</td>\n",
       "      <td>308.519989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>24.161013</td>\n",
       "      <td>42.018002</td>\n",
       "      <td>512.463013</td>\n",
       "      <td>36.380001</td>\n",
       "      <td>308.380005</td>\n",
       "      <td>300.850006</td>\n",
       "      <td>307.010010</td>\n",
       "      <td>302.190002</td>\n",
       "      <td>2774200</td>\n",
       "      <td>302.190002</td>\n",
       "      <td>305.354996</td>\n",
       "      <td>-9000.0</td>\n",
       "      <td>-6.329987</td>\n",
       "      <td>308.519989</td>\n",
       "      <td>-0.020517</td>\n",
       "      <td>0.042016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>24.163294</td>\n",
       "      <td>42.256001</td>\n",
       "      <td>500.585632</td>\n",
       "      <td>38.759998</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>292.380005</td>\n",
       "      <td>302.239990</td>\n",
       "      <td>295.290009</td>\n",
       "      <td>3519000</td>\n",
       "      <td>295.290009</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>744800.0</td>\n",
       "      <td>-6.899994</td>\n",
       "      <td>302.190002</td>\n",
       "      <td>-0.022833</td>\n",
       "      <td>0.044505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>24.502111</td>\n",
       "      <td>42.189999</td>\n",
       "      <td>499.727997</td>\n",
       "      <td>37.279999</td>\n",
       "      <td>301.279999</td>\n",
       "      <td>295.329987</td>\n",
       "      <td>297.500000</td>\n",
       "      <td>298.420013</td>\n",
       "      <td>2640300</td>\n",
       "      <td>298.420013</td>\n",
       "      <td>301.105003</td>\n",
       "      <td>-878700.0</td>\n",
       "      <td>3.130005</td>\n",
       "      <td>295.290009</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.047204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>25.443539</td>\n",
       "      <td>42.124001</td>\n",
       "      <td>501.303680</td>\n",
       "      <td>39.090000</td>\n",
       "      <td>303.140015</td>\n",
       "      <td>296.109985</td>\n",
       "      <td>300.320007</td>\n",
       "      <td>300.459991</td>\n",
       "      <td>3088400</td>\n",
       "      <td>300.459991</td>\n",
       "      <td>300.976001</td>\n",
       "      <td>448100.0</td>\n",
       "      <td>2.039978</td>\n",
       "      <td>298.420013</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.050522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AAPL       TSLA        GOOG       TWTR        High  \\\n",
       "2015-01-02  24.861401  43.862000  523.373108  36.560001  314.750000   \n",
       "2015-01-05  24.161013  42.018002  512.463013  36.380001  308.380005   \n",
       "2015-01-06  24.163294  42.256001  500.585632  38.759998  303.000000   \n",
       "2015-01-07  24.502111  42.189999  499.727997  37.279999  301.279999   \n",
       "2015-01-08  25.443539  42.124001  501.303680  39.090000  303.140015   \n",
       "\n",
       "                   Low        Open       Close   Volume   Adj Close  \\\n",
       "2015-01-02  306.959991  312.579987  308.519989  2783200  308.519989   \n",
       "2015-01-05  300.850006  307.010010  302.190002  2774200  302.190002   \n",
       "2015-01-06  292.380005  302.239990  295.290009  3519000  295.290009   \n",
       "2015-01-07  295.329987  297.500000  298.420013  2640300  298.420013   \n",
       "2015-01-08  296.109985  300.320007  300.459991  3088400  300.459991   \n",
       "\n",
       "             Moving_av  Increase_in_vol  Increase_in_adj_close   PrevClose  \\\n",
       "2015-01-02  308.519989              0.0               0.000000         NaN   \n",
       "2015-01-05  305.354996          -9000.0              -6.329987  308.519989   \n",
       "2015-01-06  302.000000         744800.0              -6.899994  302.190002   \n",
       "2015-01-07  301.105003        -878700.0               3.130005  295.290009   \n",
       "2015-01-08  300.976001         448100.0               2.039978  298.420013   \n",
       "\n",
       "              Return      sent  \n",
       "2015-01-02       NaN  0.026933  \n",
       "2015-01-05 -0.020517  0.042016  \n",
       "2015-01-06 -0.022833  0.044505  \n",
       "2015-01-07  0.010600  0.047204  \n",
       "2015-01-08  0.006836  0.050522  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df = pd.read_csv('AMZN_avg_sent.csv')\n",
    "dates = []\n",
    "for i in sent_df['date']:\n",
    "    dates.append(datetime.strptime(i, '%m/%d/%y'))\n",
    "\n",
    "#convert sentiment datafram to match format of main_df so they can be joined on date\n",
    "dates = np.array((dates))\n",
    "dates = pd.DataFrame(dates)\n",
    "dates.set_index(0)\n",
    "sent_df['dates'] = dates\n",
    "sent_df = sent_df[['sent', 'dates']]\n",
    "sent_df = sent_df.set_index('dates')\n",
    "main_df_sent = main_df.join(sent_df, how='inner')\n",
    "main_df_sent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Creating the RNN Classifier****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary libarires for this RNN binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QFrR3euMu8E3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the input/feature columns and target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AmY0FkVuvhQw"
   },
   "outputs": [],
   "source": [
    "input_cols = []\n",
    "target_col = 'Return'\n",
    "exclude = ['High', 'Low', 'Open', 'Volume', 'Adj Close', 'Moving_av', 'Increase_in_vol', \"Increase_in_adj_close\"]\n",
    "for col in main_df.columns:\n",
    "    if col != target_col and col not in exclude:\n",
    "        input_cols.append(col)\n",
    "        \n",
    "inputs = main_df[input_cols].values[1:]\n",
    "targets = main_df[target_col].values[1:]\n",
    "\n",
    "inputs_sent = main_df_sent[input_cols].values[1:]\n",
    "targets_sent = main_df_sent[target_col].values[1:]\n",
    "\n",
    "T = 10 #how many days we look back to predict the next day\n",
    "D = inputs.shape[1] #number of features\n",
    "N = len(inputs) - T #number of examples\n",
    "N_sent = len(inputs_sent) - T #number of examples with sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing the data so it is more interperatble for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "Ntrain = len(inputs) * 2//3\n",
    "scaler.fit(inputs[:Ntrain + T - 1])\n",
    "inputs = scaler.transform(inputs)\n",
    "\n",
    "Ntrain_sent = len(inputs_sent) * 2//3\n",
    "scaler.fit(inputs_sent[:Ntrain_sent + T - 1])\n",
    "inputs_sent = scaler.transform(inputs_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering the X and Y training data. Y data will be 1 if the return was positive, otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((Ntrain, T, D))\n",
    "y_train = np.zeros(Ntrain)\n",
    "for t in range(Ntrain):\n",
    "    X_train[t,:,:] = inputs[t:t+T]\n",
    "    y_train[t] = (targets[t + T] > 0)\n",
    "    \n",
    "X_train_sent = np.zeros((Ntrain_sent, T, D))\n",
    "y_train_sent = np.zeros(Ntrain_sent)\n",
    "for t in range(Ntrain_sent):\n",
    "    X_train_sent[t,:,:] = inputs_sent[t:t+T]\n",
    "    y_train_sent[t] = (targets_sent[t + T] > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering X and Y test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.zeros((N - Ntrain, T, D))\n",
    "y_test = np.zeros(N - Ntrain)\n",
    "for i in range(N - Ntrain):\n",
    "    t = i + Ntrain\n",
    "    X_test[i,:,:] = inputs[t:t+T]\n",
    "    y_test[i] = (targets[t + T] > 0)\n",
    "    \n",
    "X_test_sent = np.zeros((N_sent - Ntrain_sent, T, D))\n",
    "y_test_sent = np.zeros(N_sent - Ntrain_sent)\n",
    "for i in range(N_sent - Ntrain_sent):\n",
    "    t = i + Ntrain_sent\n",
    "    X_test_sent[i,:,:] = inputs_sent[t:t+T]\n",
    "    y_test_sent[i] = (targets_sent[t + T] > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the RNN Binary Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 40\n",
    "learning_rate = 1e-5\n",
    "epochs = 150\n",
    "\n",
    "hidden_sent = 40\n",
    "learning_rate_sent = 1e-5\n",
    "epochs_sent = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "32/32 [==============================] - 2s 14ms/step - loss: 0.6993 - accuracy: 0.5034 - val_loss: 0.7045 - val_accuracy: 0.4960\n",
      "Epoch 2/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6949 - accuracy: 0.5175 - val_loss: 0.7044 - val_accuracy: 0.4960\n",
      "Epoch 3/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6987 - accuracy: 0.5033 - val_loss: 0.7043 - val_accuracy: 0.4980\n",
      "Epoch 4/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6988 - accuracy: 0.4978 - val_loss: 0.7042 - val_accuracy: 0.4980\n",
      "Epoch 5/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.7000 - accuracy: 0.4998 - val_loss: 0.7040 - val_accuracy: 0.4939\n",
      "Epoch 6/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.7015 - accuracy: 0.4983 - val_loss: 0.7039 - val_accuracy: 0.4939\n",
      "Epoch 7/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.7033 - accuracy: 0.4759 - val_loss: 0.7038 - val_accuracy: 0.4939\n",
      "Epoch 8/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6990 - accuracy: 0.4873 - val_loss: 0.7038 - val_accuracy: 0.4939\n",
      "Epoch 9/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6978 - accuracy: 0.4922 - val_loss: 0.7037 - val_accuracy: 0.4980\n",
      "Epoch 10/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6986 - accuracy: 0.4888 - val_loss: 0.7036 - val_accuracy: 0.4980\n",
      "Epoch 11/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6966 - accuracy: 0.5168 - val_loss: 0.7035 - val_accuracy: 0.5000\n",
      "Epoch 12/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.7042 - accuracy: 0.4713 - val_loss: 0.7034 - val_accuracy: 0.5000\n",
      "Epoch 13/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6961 - accuracy: 0.5029 - val_loss: 0.7034 - val_accuracy: 0.4960\n",
      "Epoch 14/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6985 - accuracy: 0.4927 - val_loss: 0.7033 - val_accuracy: 0.4980\n",
      "Epoch 15/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.7000 - accuracy: 0.4875 - val_loss: 0.7032 - val_accuracy: 0.4939\n",
      "Epoch 16/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6988 - accuracy: 0.4853 - val_loss: 0.7031 - val_accuracy: 0.4960\n",
      "Epoch 17/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5069 - val_loss: 0.7031 - val_accuracy: 0.4919\n",
      "Epoch 18/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6976 - accuracy: 0.4836 - val_loss: 0.7030 - val_accuracy: 0.4919\n",
      "Epoch 19/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6964 - accuracy: 0.5040 - val_loss: 0.7029 - val_accuracy: 0.4899\n",
      "Epoch 20/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6953 - accuracy: 0.4997 - val_loss: 0.7028 - val_accuracy: 0.4879\n",
      "Epoch 21/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6948 - accuracy: 0.4925 - val_loss: 0.7027 - val_accuracy: 0.4879\n",
      "Epoch 22/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6949 - accuracy: 0.5180 - val_loss: 0.7027 - val_accuracy: 0.4879\n",
      "Epoch 23/150\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6943 - accuracy: 0.5226 - val_loss: 0.7026 - val_accuracy: 0.4879\n",
      "Epoch 24/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6946 - accuracy: 0.5360 - val_loss: 0.7024 - val_accuracy: 0.4879\n",
      "Epoch 25/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5411 - val_loss: 0.7024 - val_accuracy: 0.4879\n",
      "Epoch 26/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6950 - accuracy: 0.5235 - val_loss: 0.7024 - val_accuracy: 0.4899\n",
      "Epoch 27/150\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6923 - accuracy: 0.5371 - val_loss: 0.7023 - val_accuracy: 0.4899\n",
      "Epoch 28/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6947 - accuracy: 0.5356 - val_loss: 0.7022 - val_accuracy: 0.4919\n",
      "Epoch 29/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6971 - accuracy: 0.5084 - val_loss: 0.7022 - val_accuracy: 0.4919\n",
      "Epoch 30/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6958 - accuracy: 0.5160 - val_loss: 0.7021 - val_accuracy: 0.4960\n",
      "Epoch 31/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6939 - accuracy: 0.5202 - val_loss: 0.7020 - val_accuracy: 0.4960\n",
      "Epoch 32/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6940 - accuracy: 0.5266 - val_loss: 0.7020 - val_accuracy: 0.4939\n",
      "Epoch 33/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.5193 - val_loss: 0.7020 - val_accuracy: 0.5020\n",
      "Epoch 34/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6939 - accuracy: 0.5293 - val_loss: 0.7019 - val_accuracy: 0.5162\n",
      "Epoch 35/150\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6920 - accuracy: 0.5199 - val_loss: 0.7018 - val_accuracy: 0.5162\n",
      "Epoch 36/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6964 - accuracy: 0.5192 - val_loss: 0.7017 - val_accuracy: 0.5162\n",
      "Epoch 37/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6922 - accuracy: 0.5235 - val_loss: 0.7017 - val_accuracy: 0.5162\n",
      "Epoch 38/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6924 - accuracy: 0.5358 - val_loss: 0.7016 - val_accuracy: 0.5162\n",
      "Epoch 39/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5207 - val_loss: 0.7016 - val_accuracy: 0.5162\n",
      "Epoch 40/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5028 - val_loss: 0.7016 - val_accuracy: 0.5162\n",
      "Epoch 41/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6910 - accuracy: 0.5406 - val_loss: 0.7015 - val_accuracy: 0.5182\n",
      "Epoch 42/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6926 - accuracy: 0.5033 - val_loss: 0.7015 - val_accuracy: 0.5182\n",
      "Epoch 43/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.7014 - val_accuracy: 0.5182\n",
      "Epoch 44/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6912 - accuracy: 0.5355 - val_loss: 0.7014 - val_accuracy: 0.5182\n",
      "Epoch 45/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6910 - accuracy: 0.5367 - val_loss: 0.7013 - val_accuracy: 0.5182\n",
      "Epoch 46/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6898 - accuracy: 0.5394 - val_loss: 0.7013 - val_accuracy: 0.5182\n",
      "Epoch 47/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6901 - accuracy: 0.5350 - val_loss: 0.7012 - val_accuracy: 0.5162\n",
      "Epoch 48/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6895 - accuracy: 0.5432 - val_loss: 0.7012 - val_accuracy: 0.5182\n",
      "Epoch 49/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6891 - accuracy: 0.5503 - val_loss: 0.7012 - val_accuracy: 0.5182\n",
      "Epoch 50/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6900 - accuracy: 0.5430 - val_loss: 0.7012 - val_accuracy: 0.5182\n",
      "Epoch 51/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6913 - accuracy: 0.5447 - val_loss: 0.7011 - val_accuracy: 0.5182\n",
      "Epoch 52/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6928 - accuracy: 0.5326 - val_loss: 0.7011 - val_accuracy: 0.5182\n",
      "Epoch 53/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.5438 - val_loss: 0.7010 - val_accuracy: 0.5182\n",
      "Epoch 54/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.5516 - val_loss: 0.7011 - val_accuracy: 0.5243\n",
      "Epoch 55/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.5529 - val_loss: 0.7010 - val_accuracy: 0.5243\n",
      "Epoch 56/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5354 - val_loss: 0.7009 - val_accuracy: 0.5243\n",
      "Epoch 57/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6909 - accuracy: 0.5403 - val_loss: 0.7009 - val_accuracy: 0.5243\n",
      "Epoch 58/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6880 - accuracy: 0.5572 - val_loss: 0.7009 - val_accuracy: 0.5243\n",
      "Epoch 59/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6883 - accuracy: 0.5579 - val_loss: 0.7008 - val_accuracy: 0.5243\n",
      "Epoch 60/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5448 - val_loss: 0.7007 - val_accuracy: 0.5243\n",
      "Epoch 61/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6899 - accuracy: 0.5477 - val_loss: 0.7007 - val_accuracy: 0.5263\n",
      "Epoch 62/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6846 - accuracy: 0.5919 - val_loss: 0.7007 - val_accuracy: 0.5263\n",
      "Epoch 63/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6887 - accuracy: 0.5535 - val_loss: 0.7007 - val_accuracy: 0.5263\n",
      "Epoch 64/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6913 - accuracy: 0.5303 - val_loss: 0.7006 - val_accuracy: 0.5263\n",
      "Epoch 65/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6879 - accuracy: 0.5559 - val_loss: 0.7006 - val_accuracy: 0.5263\n",
      "Epoch 66/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6896 - accuracy: 0.5393 - val_loss: 0.7005 - val_accuracy: 0.5263\n",
      "Epoch 67/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6899 - accuracy: 0.5395 - val_loss: 0.7006 - val_accuracy: 0.5263\n",
      "Epoch 68/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6906 - accuracy: 0.5372 - val_loss: 0.7006 - val_accuracy: 0.5263\n",
      "Epoch 69/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6914 - accuracy: 0.5430 - val_loss: 0.7004 - val_accuracy: 0.5263\n",
      "Epoch 70/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5578 - val_loss: 0.7004 - val_accuracy: 0.5263\n",
      "Epoch 71/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6888 - accuracy: 0.5618 - val_loss: 0.7003 - val_accuracy: 0.5263\n",
      "Epoch 72/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6901 - accuracy: 0.5395 - val_loss: 0.7003 - val_accuracy: 0.5263\n",
      "Epoch 73/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6886 - accuracy: 0.5406 - val_loss: 0.7003 - val_accuracy: 0.5263\n",
      "Epoch 74/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6905 - accuracy: 0.5492 - val_loss: 0.7003 - val_accuracy: 0.5263\n",
      "Epoch 75/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6889 - accuracy: 0.5403 - val_loss: 0.7002 - val_accuracy: 0.5263\n",
      "Epoch 76/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6900 - accuracy: 0.5381 - val_loss: 0.7002 - val_accuracy: 0.5263\n",
      "Epoch 77/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6881 - accuracy: 0.5645 - val_loss: 0.7001 - val_accuracy: 0.5263\n",
      "Epoch 78/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6888 - accuracy: 0.5536 - val_loss: 0.7001 - val_accuracy: 0.5263\n",
      "Epoch 79/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6918 - accuracy: 0.5315 - val_loss: 0.7001 - val_accuracy: 0.5263\n",
      "Epoch 80/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6849 - accuracy: 0.5698 - val_loss: 0.7000 - val_accuracy: 0.5263\n",
      "Epoch 81/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6895 - accuracy: 0.5562 - val_loss: 0.7000 - val_accuracy: 0.5283\n",
      "Epoch 82/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6880 - accuracy: 0.5478 - val_loss: 0.7000 - val_accuracy: 0.5283\n",
      "Epoch 83/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6896 - accuracy: 0.5418 - val_loss: 0.7000 - val_accuracy: 0.5283\n",
      "Epoch 84/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6910 - accuracy: 0.5481 - val_loss: 0.6999 - val_accuracy: 0.5263\n",
      "Epoch 85/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6886 - accuracy: 0.5409 - val_loss: 0.6999 - val_accuracy: 0.5283\n",
      "Epoch 86/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6921 - accuracy: 0.5301 - val_loss: 0.6998 - val_accuracy: 0.5283\n",
      "Epoch 87/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6868 - accuracy: 0.5512 - val_loss: 0.6999 - val_accuracy: 0.5283\n",
      "Epoch 88/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6880 - accuracy: 0.5636 - val_loss: 0.6998 - val_accuracy: 0.5283\n",
      "Epoch 89/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6893 - accuracy: 0.5475 - val_loss: 0.6998 - val_accuracy: 0.5283\n",
      "Epoch 90/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5679 - val_loss: 0.6998 - val_accuracy: 0.5283\n",
      "Epoch 91/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6838 - accuracy: 0.5784 - val_loss: 0.6997 - val_accuracy: 0.5283\n",
      "Epoch 92/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6881 - accuracy: 0.5447 - val_loss: 0.6996 - val_accuracy: 0.5283\n",
      "Epoch 93/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6901 - accuracy: 0.5384 - val_loss: 0.6996 - val_accuracy: 0.5283\n",
      "Epoch 94/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6868 - accuracy: 0.5602 - val_loss: 0.6995 - val_accuracy: 0.5263\n",
      "Epoch 95/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6876 - accuracy: 0.5571 - val_loss: 0.6995 - val_accuracy: 0.5263\n",
      "Epoch 96/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6872 - accuracy: 0.5474 - val_loss: 0.6995 - val_accuracy: 0.5263\n",
      "Epoch 97/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6910 - accuracy: 0.5443 - val_loss: 0.6995 - val_accuracy: 0.5263\n",
      "Epoch 98/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6907 - accuracy: 0.5432 - val_loss: 0.6995 - val_accuracy: 0.5263\n",
      "Epoch 99/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6904 - accuracy: 0.5432 - val_loss: 0.6994 - val_accuracy: 0.5263\n",
      "Epoch 100/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6896 - accuracy: 0.5321 - val_loss: 0.6994 - val_accuracy: 0.5263\n",
      "Epoch 101/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6904 - accuracy: 0.5406 - val_loss: 0.6994 - val_accuracy: 0.5263\n",
      "Epoch 102/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6901 - accuracy: 0.5276 - val_loss: 0.6994 - val_accuracy: 0.5263\n",
      "Epoch 103/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6881 - accuracy: 0.5462 - val_loss: 0.6994 - val_accuracy: 0.5263\n",
      "Epoch 104/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6845 - accuracy: 0.5608 - val_loss: 0.6994 - val_accuracy: 0.5263\n",
      "Epoch 105/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6879 - accuracy: 0.5507 - val_loss: 0.6994 - val_accuracy: 0.5263\n",
      "Epoch 106/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6899 - accuracy: 0.5397 - val_loss: 0.6994 - val_accuracy: 0.5263\n",
      "Epoch 107/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6868 - accuracy: 0.5456 - val_loss: 0.6993 - val_accuracy: 0.5263\n",
      "Epoch 108/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6892 - accuracy: 0.5548 - val_loss: 0.6993 - val_accuracy: 0.5263\n",
      "Epoch 109/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6831 - accuracy: 0.5719 - val_loss: 0.6993 - val_accuracy: 0.5283\n",
      "Epoch 110/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6856 - accuracy: 0.5616 - val_loss: 0.6994 - val_accuracy: 0.5263\n",
      "Epoch 111/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6889 - accuracy: 0.5397 - val_loss: 0.6994 - val_accuracy: 0.5263\n",
      "Epoch 112/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5549 - val_loss: 0.6994 - val_accuracy: 0.5263\n",
      "Epoch 113/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6900 - accuracy: 0.5367 - val_loss: 0.6994 - val_accuracy: 0.5263\n",
      "Epoch 114/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6880 - accuracy: 0.5465 - val_loss: 0.6993 - val_accuracy: 0.5263\n",
      "Epoch 115/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6844 - accuracy: 0.5603 - val_loss: 0.6994 - val_accuracy: 0.5263\n",
      "Epoch 116/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6884 - accuracy: 0.5432 - val_loss: 0.6993 - val_accuracy: 0.5263\n",
      "Epoch 117/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6884 - accuracy: 0.5562 - val_loss: 0.6992 - val_accuracy: 0.5263\n",
      "Epoch 118/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6822 - accuracy: 0.5751 - val_loss: 0.6993 - val_accuracy: 0.5263\n",
      "Epoch 119/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6870 - accuracy: 0.5647 - val_loss: 0.6993 - val_accuracy: 0.5263\n",
      "Epoch 120/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.5480 - val_loss: 0.6993 - val_accuracy: 0.5263\n",
      "Epoch 121/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6884 - accuracy: 0.5444 - val_loss: 0.6993 - val_accuracy: 0.5263\n",
      "Epoch 122/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6881 - accuracy: 0.5463 - val_loss: 0.6993 - val_accuracy: 0.5263\n",
      "Epoch 123/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6848 - accuracy: 0.5663 - val_loss: 0.6992 - val_accuracy: 0.5263\n",
      "Epoch 124/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6851 - accuracy: 0.5630 - val_loss: 0.6992 - val_accuracy: 0.5263\n",
      "Epoch 125/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6888 - accuracy: 0.5463 - val_loss: 0.6992 - val_accuracy: 0.5263\n",
      "Epoch 126/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6880 - accuracy: 0.5541 - val_loss: 0.6992 - val_accuracy: 0.5263\n",
      "Epoch 127/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6873 - accuracy: 0.5551 - val_loss: 0.6992 - val_accuracy: 0.5263\n",
      "Epoch 128/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6884 - accuracy: 0.5402 - val_loss: 0.6992 - val_accuracy: 0.5263\n",
      "Epoch 129/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6845 - accuracy: 0.5622 - val_loss: 0.6992 - val_accuracy: 0.5263\n",
      "Epoch 130/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6858 - accuracy: 0.5560 - val_loss: 0.6992 - val_accuracy: 0.5263\n",
      "Epoch 131/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6853 - accuracy: 0.5630 - val_loss: 0.6992 - val_accuracy: 0.5263\n",
      "Epoch 132/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5603 - val_loss: 0.6991 - val_accuracy: 0.5263\n",
      "Epoch 133/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.5337 - val_loss: 0.6991 - val_accuracy: 0.5263\n",
      "Epoch 134/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6830 - accuracy: 0.5737 - val_loss: 0.6991 - val_accuracy: 0.5263\n",
      "Epoch 135/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5489 - val_loss: 0.6991 - val_accuracy: 0.5263\n",
      "Epoch 136/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6856 - accuracy: 0.5532 - val_loss: 0.6991 - val_accuracy: 0.5263\n",
      "Epoch 137/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6857 - accuracy: 0.5650 - val_loss: 0.6991 - val_accuracy: 0.5263\n",
      "Epoch 138/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6901 - accuracy: 0.5389 - val_loss: 0.6991 - val_accuracy: 0.5263\n",
      "Epoch 139/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6890 - accuracy: 0.5357 - val_loss: 0.6991 - val_accuracy: 0.5263\n",
      "Epoch 140/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5533 - val_loss: 0.6991 - val_accuracy: 0.5263\n",
      "Epoch 141/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5544 - val_loss: 0.6991 - val_accuracy: 0.5263\n",
      "Epoch 142/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6844 - accuracy: 0.5572 - val_loss: 0.6991 - val_accuracy: 0.5263\n",
      "Epoch 143/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6887 - accuracy: 0.5413 - val_loss: 0.6991 - val_accuracy: 0.5263\n",
      "Epoch 144/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6895 - accuracy: 0.5386 - val_loss: 0.6991 - val_accuracy: 0.5263\n",
      "Epoch 145/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6893 - accuracy: 0.5359 - val_loss: 0.6990 - val_accuracy: 0.5263\n",
      "Epoch 146/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6839 - accuracy: 0.5561 - val_loss: 0.6990 - val_accuracy: 0.5263\n",
      "Epoch 147/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6823 - accuracy: 0.5708 - val_loss: 0.6991 - val_accuracy: 0.5263\n",
      "Epoch 148/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6848 - accuracy: 0.5642 - val_loss: 0.6990 - val_accuracy: 0.5263\n",
      "Epoch 149/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6880 - accuracy: 0.5487 - val_loss: 0.6990 - val_accuracy: 0.5263\n",
      "Epoch 150/150\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6842 - accuracy: 0.5482 - val_loss: 0.6990 - val_accuracy: 0.5263\n",
      "Epoch 1/150\n",
      "27/27 [==============================] - 2s 16ms/step - loss: 0.6930 - accuracy: 0.5043 - val_loss: 0.7087 - val_accuracy: 0.4597\n",
      "Epoch 2/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6988 - accuracy: 0.4689 - val_loss: 0.7083 - val_accuracy: 0.4572\n",
      "Epoch 3/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6952 - accuracy: 0.4780 - val_loss: 0.7079 - val_accuracy: 0.4572\n",
      "Epoch 4/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6965 - accuracy: 0.4819 - val_loss: 0.7076 - val_accuracy: 0.4572\n",
      "Epoch 5/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6939 - accuracy: 0.4961 - val_loss: 0.7070 - val_accuracy: 0.4597\n",
      "Epoch 6/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6951 - accuracy: 0.4873 - val_loss: 0.7067 - val_accuracy: 0.4597\n",
      "Epoch 7/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6995 - accuracy: 0.4561 - val_loss: 0.7062 - val_accuracy: 0.4621\n",
      "Epoch 8/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6954 - accuracy: 0.4893 - val_loss: 0.7058 - val_accuracy: 0.4645\n",
      "Epoch 9/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6891 - accuracy: 0.5221 - val_loss: 0.7056 - val_accuracy: 0.4645\n",
      "Epoch 10/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6957 - accuracy: 0.4858 - val_loss: 0.7052 - val_accuracy: 0.4670\n",
      "Epoch 11/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5085 - val_loss: 0.7048 - val_accuracy: 0.4694\n",
      "Epoch 12/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6909 - accuracy: 0.5057 - val_loss: 0.7042 - val_accuracy: 0.4621\n",
      "Epoch 13/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.4972 - val_loss: 0.7039 - val_accuracy: 0.4597\n",
      "Epoch 14/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6968 - accuracy: 0.4768 - val_loss: 0.7035 - val_accuracy: 0.4572\n",
      "Epoch 15/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5126 - val_loss: 0.7032 - val_accuracy: 0.4572\n",
      "Epoch 16/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6949 - accuracy: 0.5073 - val_loss: 0.7027 - val_accuracy: 0.4572\n",
      "Epoch 17/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6915 - accuracy: 0.5355 - val_loss: 0.7023 - val_accuracy: 0.4523\n",
      "Epoch 18/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6937 - accuracy: 0.5023 - val_loss: 0.7018 - val_accuracy: 0.4474\n",
      "Epoch 19/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6922 - accuracy: 0.5138 - val_loss: 0.7014 - val_accuracy: 0.4474\n",
      "Epoch 20/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6945 - accuracy: 0.5068 - val_loss: 0.7008 - val_accuracy: 0.4597\n",
      "Epoch 21/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6902 - accuracy: 0.5385 - val_loss: 0.7005 - val_accuracy: 0.4572\n",
      "Epoch 22/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6945 - accuracy: 0.5263 - val_loss: 0.7003 - val_accuracy: 0.4597\n",
      "Epoch 23/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6938 - accuracy: 0.5244 - val_loss: 0.6999 - val_accuracy: 0.4597\n",
      "Epoch 24/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5119 - val_loss: 0.6996 - val_accuracy: 0.4597\n",
      "Epoch 25/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6946 - accuracy: 0.5313 - val_loss: 0.6992 - val_accuracy: 0.4621\n",
      "Epoch 26/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6900 - accuracy: 0.5427 - val_loss: 0.6989 - val_accuracy: 0.4597\n",
      "Epoch 27/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5425 - val_loss: 0.6987 - val_accuracy: 0.4572\n",
      "Epoch 28/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6883 - accuracy: 0.5712 - val_loss: 0.6984 - val_accuracy: 0.4572\n",
      "Epoch 29/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5472 - val_loss: 0.6982 - val_accuracy: 0.4572\n",
      "Epoch 30/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.5532 - val_loss: 0.6979 - val_accuracy: 0.4572\n",
      "Epoch 31/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6895 - accuracy: 0.5532 - val_loss: 0.6976 - val_accuracy: 0.4572\n",
      "Epoch 32/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6913 - accuracy: 0.5556 - val_loss: 0.6973 - val_accuracy: 0.4572\n",
      "Epoch 33/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6893 - accuracy: 0.5504 - val_loss: 0.6971 - val_accuracy: 0.4572\n",
      "Epoch 34/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5634 - val_loss: 0.6968 - val_accuracy: 0.4597\n",
      "Epoch 35/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6887 - accuracy: 0.5759 - val_loss: 0.6966 - val_accuracy: 0.4548\n",
      "Epoch 36/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6894 - accuracy: 0.5669 - val_loss: 0.6965 - val_accuracy: 0.4597\n",
      "Epoch 37/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6853 - accuracy: 0.5805 - val_loss: 0.6964 - val_accuracy: 0.4597\n",
      "Epoch 38/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6899 - accuracy: 0.5409 - val_loss: 0.6961 - val_accuracy: 0.4670\n",
      "Epoch 39/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5475 - val_loss: 0.6959 - val_accuracy: 0.4719\n",
      "Epoch 40/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6912 - accuracy: 0.5506 - val_loss: 0.6957 - val_accuracy: 0.4670\n",
      "Epoch 41/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6908 - accuracy: 0.5538 - val_loss: 0.6955 - val_accuracy: 0.4719\n",
      "Epoch 42/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6883 - accuracy: 0.5667 - val_loss: 0.6953 - val_accuracy: 0.4719\n",
      "Epoch 43/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5704 - val_loss: 0.6952 - val_accuracy: 0.4719\n",
      "Epoch 44/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5402 - val_loss: 0.6951 - val_accuracy: 0.4743\n",
      "Epoch 45/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6870 - accuracy: 0.5750 - val_loss: 0.6949 - val_accuracy: 0.4719\n",
      "Epoch 46/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6882 - accuracy: 0.5570 - val_loss: 0.6948 - val_accuracy: 0.4768\n",
      "Epoch 47/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6882 - accuracy: 0.5503 - val_loss: 0.6946 - val_accuracy: 0.4939\n",
      "Epoch 48/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6875 - accuracy: 0.5743 - val_loss: 0.6945 - val_accuracy: 0.4939\n",
      "Epoch 49/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.5416 - val_loss: 0.6944 - val_accuracy: 0.5012\n",
      "Epoch 50/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6903 - accuracy: 0.5562 - val_loss: 0.6942 - val_accuracy: 0.5012\n",
      "Epoch 51/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6896 - accuracy: 0.5486 - val_loss: 0.6940 - val_accuracy: 0.4963\n",
      "Epoch 52/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6855 - accuracy: 0.5663 - val_loss: 0.6938 - val_accuracy: 0.4914\n",
      "Epoch 53/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6902 - accuracy: 0.5697 - val_loss: 0.6937 - val_accuracy: 0.4988\n",
      "Epoch 54/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6874 - accuracy: 0.5759 - val_loss: 0.6937 - val_accuracy: 0.5110\n",
      "Epoch 55/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6853 - accuracy: 0.5818 - val_loss: 0.6935 - val_accuracy: 0.5208\n",
      "Epoch 56/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6873 - accuracy: 0.5529 - val_loss: 0.6933 - val_accuracy: 0.5306\n",
      "Epoch 57/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6916 - accuracy: 0.5669 - val_loss: 0.6932 - val_accuracy: 0.5257\n",
      "Epoch 58/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6881 - accuracy: 0.5698 - val_loss: 0.6931 - val_accuracy: 0.5306\n",
      "Epoch 59/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6892 - accuracy: 0.5497 - val_loss: 0.6929 - val_accuracy: 0.5355\n",
      "Epoch 60/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6910 - accuracy: 0.5500 - val_loss: 0.6928 - val_accuracy: 0.5257\n",
      "Epoch 61/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6874 - accuracy: 0.5651 - val_loss: 0.6927 - val_accuracy: 0.5208\n",
      "Epoch 62/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6871 - accuracy: 0.5736 - val_loss: 0.6926 - val_accuracy: 0.5208\n",
      "Epoch 63/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6940 - accuracy: 0.5372 - val_loss: 0.6924 - val_accuracy: 0.5281\n",
      "Epoch 64/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6900 - accuracy: 0.5547 - val_loss: 0.6923 - val_accuracy: 0.5208\n",
      "Epoch 65/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6881 - accuracy: 0.5689 - val_loss: 0.6923 - val_accuracy: 0.5232\n",
      "Epoch 66/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6900 - accuracy: 0.5521 - val_loss: 0.6923 - val_accuracy: 0.5232\n",
      "Epoch 67/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6896 - accuracy: 0.5454 - val_loss: 0.6922 - val_accuracy: 0.5379\n",
      "Epoch 68/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6911 - accuracy: 0.5454 - val_loss: 0.6921 - val_accuracy: 0.5501\n",
      "Epoch 69/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5301 - val_loss: 0.6920 - val_accuracy: 0.5477\n",
      "Epoch 70/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6931 - accuracy: 0.5362 - val_loss: 0.6920 - val_accuracy: 0.5477\n",
      "Epoch 71/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6893 - accuracy: 0.5490 - val_loss: 0.6919 - val_accuracy: 0.5452\n",
      "Epoch 72/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6827 - accuracy: 0.5979 - val_loss: 0.6919 - val_accuracy: 0.5403\n",
      "Epoch 73/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6930 - accuracy: 0.5431 - val_loss: 0.6919 - val_accuracy: 0.5428\n",
      "Epoch 74/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6873 - accuracy: 0.5769 - val_loss: 0.6918 - val_accuracy: 0.5355\n",
      "Epoch 75/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6878 - accuracy: 0.5727 - val_loss: 0.6918 - val_accuracy: 0.5355\n",
      "Epoch 76/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6884 - accuracy: 0.5629 - val_loss: 0.6918 - val_accuracy: 0.5379\n",
      "Epoch 77/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6879 - accuracy: 0.5521 - val_loss: 0.6917 - val_accuracy: 0.5403\n",
      "Epoch 78/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6895 - accuracy: 0.5578 - val_loss: 0.6917 - val_accuracy: 0.5403\n",
      "Epoch 79/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6877 - accuracy: 0.5568 - val_loss: 0.6917 - val_accuracy: 0.5403\n",
      "Epoch 80/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6878 - accuracy: 0.5606 - val_loss: 0.6916 - val_accuracy: 0.5379\n",
      "Epoch 81/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6855 - accuracy: 0.5723 - val_loss: 0.6916 - val_accuracy: 0.5379\n",
      "Epoch 82/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6886 - accuracy: 0.5600 - val_loss: 0.6916 - val_accuracy: 0.5428\n",
      "Epoch 83/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6866 - accuracy: 0.5545 - val_loss: 0.6915 - val_accuracy: 0.5428\n",
      "Epoch 84/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6881 - accuracy: 0.5404 - val_loss: 0.6915 - val_accuracy: 0.5428\n",
      "Epoch 85/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6946 - accuracy: 0.5321 - val_loss: 0.6915 - val_accuracy: 0.5452\n",
      "Epoch 86/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5586 - val_loss: 0.6915 - val_accuracy: 0.5452\n",
      "Epoch 87/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6880 - accuracy: 0.5532 - val_loss: 0.6915 - val_accuracy: 0.5452\n",
      "Epoch 88/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6855 - accuracy: 0.5669 - val_loss: 0.6915 - val_accuracy: 0.5452\n",
      "Epoch 89/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6847 - accuracy: 0.5681 - val_loss: 0.6915 - val_accuracy: 0.5428\n",
      "Epoch 90/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6877 - accuracy: 0.5486 - val_loss: 0.6915 - val_accuracy: 0.5428\n",
      "Epoch 91/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6918 - accuracy: 0.5424 - val_loss: 0.6914 - val_accuracy: 0.5428\n",
      "Epoch 92/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6837 - accuracy: 0.5749 - val_loss: 0.6914 - val_accuracy: 0.5428\n",
      "Epoch 93/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6829 - accuracy: 0.5846 - val_loss: 0.6914 - val_accuracy: 0.5403\n",
      "Epoch 94/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6882 - accuracy: 0.5483 - val_loss: 0.6914 - val_accuracy: 0.5403\n",
      "Epoch 95/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6835 - accuracy: 0.5813 - val_loss: 0.6914 - val_accuracy: 0.5403\n",
      "Epoch 96/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.5322 - val_loss: 0.6914 - val_accuracy: 0.5403\n",
      "Epoch 97/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6853 - accuracy: 0.5532 - val_loss: 0.6914 - val_accuracy: 0.5403\n",
      "Epoch 98/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6852 - accuracy: 0.5568 - val_loss: 0.6914 - val_accuracy: 0.5403\n",
      "Epoch 99/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6842 - accuracy: 0.5617 - val_loss: 0.6914 - val_accuracy: 0.5403\n",
      "Epoch 100/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6902 - accuracy: 0.5504 - val_loss: 0.6914 - val_accuracy: 0.5403\n",
      "Epoch 101/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.5876 - val_loss: 0.6914 - val_accuracy: 0.5403\n",
      "Epoch 102/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6825 - accuracy: 0.5714 - val_loss: 0.6914 - val_accuracy: 0.5403\n",
      "Epoch 103/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6871 - accuracy: 0.5552 - val_loss: 0.6914 - val_accuracy: 0.5403\n",
      "Epoch 104/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5465 - val_loss: 0.6914 - val_accuracy: 0.5403\n",
      "Epoch 105/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6892 - accuracy: 0.5626 - val_loss: 0.6915 - val_accuracy: 0.5403\n",
      "Epoch 106/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6886 - accuracy: 0.5431 - val_loss: 0.6915 - val_accuracy: 0.5403\n",
      "Epoch 107/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6831 - accuracy: 0.5657 - val_loss: 0.6915 - val_accuracy: 0.5403\n",
      "Epoch 108/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5577 - val_loss: 0.6915 - val_accuracy: 0.5403\n",
      "Epoch 109/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6836 - accuracy: 0.5745 - val_loss: 0.6915 - val_accuracy: 0.5403\n",
      "Epoch 110/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6892 - accuracy: 0.5541 - val_loss: 0.6916 - val_accuracy: 0.5403\n",
      "Epoch 111/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.5402 - val_loss: 0.6916 - val_accuracy: 0.5403\n",
      "Epoch 112/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6844 - accuracy: 0.5697 - val_loss: 0.6916 - val_accuracy: 0.5403\n",
      "Epoch 113/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6868 - accuracy: 0.5671 - val_loss: 0.6916 - val_accuracy: 0.5403\n",
      "Epoch 114/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6830 - accuracy: 0.5786 - val_loss: 0.6917 - val_accuracy: 0.5403\n",
      "Epoch 115/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6852 - accuracy: 0.5642 - val_loss: 0.6917 - val_accuracy: 0.5403\n",
      "Epoch 116/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6836 - accuracy: 0.5737 - val_loss: 0.6918 - val_accuracy: 0.5403\n",
      "Epoch 117/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5681 - val_loss: 0.6918 - val_accuracy: 0.5403\n",
      "Epoch 118/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6891 - accuracy: 0.5507 - val_loss: 0.6919 - val_accuracy: 0.5403\n",
      "Epoch 119/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6817 - accuracy: 0.5689 - val_loss: 0.6919 - val_accuracy: 0.5403\n",
      "Epoch 120/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6838 - accuracy: 0.5678 - val_loss: 0.6919 - val_accuracy: 0.5403\n",
      "Epoch 121/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6900 - accuracy: 0.5375 - val_loss: 0.6920 - val_accuracy: 0.5403\n",
      "Epoch 122/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6848 - accuracy: 0.5575 - val_loss: 0.6920 - val_accuracy: 0.5403\n",
      "Epoch 123/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6887 - accuracy: 0.5493 - val_loss: 0.6920 - val_accuracy: 0.5403\n",
      "Epoch 124/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6797 - accuracy: 0.5841 - val_loss: 0.6921 - val_accuracy: 0.5403\n",
      "Epoch 125/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6893 - accuracy: 0.5451 - val_loss: 0.6922 - val_accuracy: 0.5403\n",
      "Epoch 126/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5646 - val_loss: 0.6922 - val_accuracy: 0.5403\n",
      "Epoch 127/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6891 - accuracy: 0.5513 - val_loss: 0.6923 - val_accuracy: 0.5403\n",
      "Epoch 128/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6872 - accuracy: 0.5563 - val_loss: 0.6923 - val_accuracy: 0.5403\n",
      "Epoch 129/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6858 - accuracy: 0.5674 - val_loss: 0.6924 - val_accuracy: 0.5403\n",
      "Epoch 130/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5469 - val_loss: 0.6924 - val_accuracy: 0.5403\n",
      "Epoch 131/150\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.6872 - accuracy: 0.5558 - val_loss: 0.6924 - val_accuracy: 0.5403\n",
      "Epoch 132/150\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.6880 - accuracy: 0.5507 - val_loss: 0.6925 - val_accuracy: 0.5403\n",
      "Epoch 133/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6912 - accuracy: 0.5297 - val_loss: 0.6926 - val_accuracy: 0.5403\n",
      "Epoch 134/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6835 - accuracy: 0.5683 - val_loss: 0.6925 - val_accuracy: 0.5403\n",
      "Epoch 135/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6830 - accuracy: 0.5633 - val_loss: 0.6926 - val_accuracy: 0.5403\n",
      "Epoch 136/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6932 - accuracy: 0.5317 - val_loss: 0.6925 - val_accuracy: 0.5403\n",
      "Epoch 137/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6873 - accuracy: 0.5570 - val_loss: 0.6926 - val_accuracy: 0.5403\n",
      "Epoch 138/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6837 - accuracy: 0.5650 - val_loss: 0.6926 - val_accuracy: 0.5403\n",
      "Epoch 139/150\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.6833 - accuracy: 0.5614 - val_loss: 0.6926 - val_accuracy: 0.5403\n",
      "Epoch 140/150\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5557 - val_loss: 0.6926 - val_accuracy: 0.5403\n",
      "Epoch 141/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6866 - accuracy: 0.5496 - val_loss: 0.6926 - val_accuracy: 0.5403\n",
      "Epoch 142/150\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.6818 - accuracy: 0.5673 - val_loss: 0.6926 - val_accuracy: 0.5403\n",
      "Epoch 143/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6898 - accuracy: 0.5436 - val_loss: 0.6926 - val_accuracy: 0.5403\n",
      "Epoch 144/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6823 - accuracy: 0.5720 - val_loss: 0.6926 - val_accuracy: 0.5403\n",
      "Epoch 145/150\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.6822 - accuracy: 0.5719 - val_loss: 0.6927 - val_accuracy: 0.5403\n",
      "Epoch 146/150\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.6894 - accuracy: 0.5376 - val_loss: 0.6928 - val_accuracy: 0.5403\n",
      "Epoch 147/150\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 0.6850 - accuracy: 0.5622 - val_loss: 0.6927 - val_accuracy: 0.5403\n",
      "Epoch 148/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6867 - accuracy: 0.5583 - val_loss: 0.6928 - val_accuracy: 0.5403\n",
      "Epoch 149/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6809 - accuracy: 0.5686 - val_loss: 0.6929 - val_accuracy: 0.5403\n",
      "Epoch 150/150\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 0.6887 - accuracy: 0.5432 - val_loss: 0.6929 - val_accuracy: 0.5403\n"
     ]
    }
   ],
   "source": [
    "i = Input(shape=(T,D))\n",
    "x = LSTM(hidden)(i) \n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(i, x)\n",
    "model.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = Adam(lr=learning_rate), #hyper parameter\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "r = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    epochs = epochs, \n",
    "    batch_size = 32,\n",
    "    validation_data = (X_test, y_test)\n",
    ")\n",
    "\n",
    "i_s = Input(shape=(T,D))\n",
    "x_s = LSTM(hidden_sent)(i_s) \n",
    "x_s = Dense(1, activation='sigmoid')(x_s)\n",
    "model_sent = Model(i_s, x_s)\n",
    "model_sent.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = Adam(lr=learning_rate_sent),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "r_sent = model_sent.fit(\n",
    "    X_train_sent, \n",
    "    y_train_sent,\n",
    "    epochs = epochs_sent, \n",
    "    batch_size = 32,\n",
    "    validation_data = (X_test_sent, y_test_sent)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the loss and accuracy of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Loss\", loc='center')\n",
    "plt.plot(r.history['loss'], 'b--', label=\"Training: W/O Sentiment\")\n",
    "plt.plot(r.history['val_loss'], 'b', label='Testing: W/O Sentiment')\n",
    "plt.plot(r_sent.history['loss'], 'r--', label=\"Training: W/ Sentiment\")\n",
    "plt.plot(r_sent.history['val_loss'], 'r', label='Testing: W/ Sentiment')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Accuracy\", loc='center')\n",
    "plt.plot(r.history['accuracy'], 'b--', label=\"Training: W/O Sentiment\")\n",
    "plt.plot(r.history['val_accuracy'], 'b', label='Testing: W/O Sentiment')\n",
    "plt.plot(r_sent.history['accuracy'], 'r--', label=\"Training: W/ Sentiment\")\n",
    "plt.plot(r_sent.history['val_accuracy'], 'r', label='Testing: W/ Sentiment')\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "StockPrediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
